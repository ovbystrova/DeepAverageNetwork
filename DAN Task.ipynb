{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9c64fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d7074",
   "metadata": {},
   "source": [
    "# Deep Average Network –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d972fe",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –º—ã –±—É–¥–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–∏—Ç—ã –Ω–∞ 3 —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.  \n",
    "–í—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤, —Ç–∞–∫ —á—Ç–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å [—Ç—É—Ç–æ—Ä–∏–∞–ª –ø–æ –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é](https://github.com/BobaZooba/DeepNLP/blob/master/Tutorials/Word%20vectors%20%26%20Data%20Loading.ipynb).\n",
    "\n",
    "–ù–∞—à–∏ –∫–ª–∞—Å—Å—ã:  \n",
    "\n",
    "–ò–Ω–¥–µ–∫—Å | Sentiment  \n",
    "-- | --  \n",
    "0 | negative  \n",
    "1 | neutral  \n",
    "2 | positive  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55010212",
   "metadata": {},
   "source": [
    "–í–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å:\n",
    "![–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ DAN](https://www.researchgate.net/profile/Shervin-Minaee/publication/340523298/figure/fig1/AS:878252264550411@1586403065555/The-architecture-of-the-Deep-Average-Network-DAN-10.ppm)\n",
    "\n",
    "–ß—Ç–æ –æ–Ω–∞ –∏–∑ —Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç:\n",
    "- –ú—ã –ø–æ–¥–∞–µ–º –≤ –Ω–µ–µ –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
    "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
    "\n",
    "–í —ç—Ç–æ–π –¥–æ–º–∞—à–∫–µ –≤–∞–º –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:\n",
    "- –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç—ã –≤ –º–∞—Ç—Ä–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "- –û–±—É—á–∏—Ç—å –µ–µ\n",
    "- –ü–æ–Ω—è—Ç—å —Ö–æ—Ä–æ—à–æ –ª–∏ –≤—ã —ç—Ç–æ —Å–¥–µ–ª–∞–ª–∏\n",
    "\n",
    "–≠—Ç–æ –æ—á–µ–Ω—å –≤–∞–∂–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–∞—è –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏. –í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞ —Ä–∞–±–æ—Ç–µ —Å–æ–≤–µ—Ç—É—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –±–µ–π–∑–ª–∞–π–Ω. –ò –≤ –∫–∞—á–µ—Å—Ç–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤ –≤–∑—è—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç –±–µ—Ä—Ç–∞/—Ä–æ–±–µ—Ä—Ç—ã/—Ç–¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65faf753",
   "metadata": {},
   "source": [
    "## ü§ó Datasets\n",
    "–í —ç—Ç–æ–º —Ç—É—Ç–æ—Ä–∏–∞–ª–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [datasets](https://github.com/huggingface/datasets). –ú—ã –≤—Ä—è–¥ –ª–∏ –µ—â–µ –±—É–¥–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —ç—Ç–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π, —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –±—É–¥–µ—Ç –≤–∞–∂–Ω–æ —Å–∞–º–∏–º –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –í–æ-–ø–µ—Ä–≤—ã—Ö, –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –≤–æ-–≤—Ç–æ—Ä—ã—Ö, –∑–¥–µ—Å—å –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–ø–ª–æ—Ö–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏. [–ó–¥–µ—Å—å](https://huggingface.co/datasets) –≤—ã —Å–º–æ–∂–µ—Ç–µ –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –æ–Ω–∏ –≤–∞–º –ø—Ä–∏–≥–æ–¥—è—Ç—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927f02a",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∏—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–ª–æ–≤\n",
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞. –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–¥–∞–≤–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤ –∏ `np.array`\n",
    "–§–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä—è:\n",
    "```python\n",
    "{\n",
    "    'aabra': 0,\n",
    "    ...,\n",
    "    'mom': 6546,\n",
    "    ...\n",
    "    'xyz': 100355\n",
    "}\n",
    "```\n",
    "–§–æ—Ä–º–∞—Ç –º–∞—Ç—Ä–∏—Ü—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\n",
    "```python\n",
    "array([[0.44442278, 0.28644582, 0.04357426, ..., 0.9425766 , 0.02024289,\n",
    "        0.88456545],\n",
    "       [0.77599317, 0.35188237, 0.54801261, ..., 0.91134102, 0.88599103,\n",
    "        0.88068835],\n",
    "       [0.68071886, 0.29352313, 0.95952505, ..., 0.19127958, 0.97723054,\n",
    "        0.36294011],\n",
    "       ...,\n",
    "       [0.03589378, 0.85429694, 0.33437761, ..., 0.39784873, 0.80368014,\n",
    "        0.76368042],\n",
    "       [0.01498725, 0.78155695, 0.80372969, ..., 0.82051826, 0.42314861,\n",
    "        0.18655465],\n",
    "       [0.69263802, 0.82090775, 0.27150426, ..., 0.86582747, 0.40896573,\n",
    "        0.33423976]])\n",
    "```\n",
    "\n",
    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –º–∞—Ç—Ä–∏—Ü–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ä–∞–∑–º–µ—Ä–æ–º —Å–ª–æ–≤–∞—Ä—è, —Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥. –ü–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É `num_tokens` –¥–æ–ª–∂–Ω–æ –±—Ä–∞—Ç—å –Ω–µ –±–æ–ª–µ–µ —É–∫–∞–∑–∞–Ω–æ –≤ —ç—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å –∏ –º–∞—Ç—Ä–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ed367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∏ —Å–∫–∞—á–∞–π—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "# !wget  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.vec.gz\n",
    "# !gzip -d cc.ru.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e15a9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path, num_tokens=None, special_tokens=[\"<PAD>\", \"<UNK>\"], verbose=True):\n",
    "    \"\"\"\n",
    "    Get mapping between token and its id in vocabulary and embeddings matrix\n",
    "    path: str\n",
    "    num_tokens: maximum number of vocabular (None if no truncation apply)\n",
    "    \n",
    "    return: token2index dictionaty, embeddigts matrix (np.array of shape  (num_tokens, emb_dim))\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path) as f:\n",
    "        num_words, emb_dim = f.readline().strip().split()\n",
    "        num_words, emb_dim = int(num_words), int(emb_dim)\n",
    "        \n",
    "        # Create initial matrix with padding token and unknown\n",
    "        token2index: Dict[str, int] = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        embeddings_matrix: np.array = np.random.rand(len(token2index), emb_dim)\n",
    "        print(f\"Created embedding matrix with shape {embeddings_matrix.shape}\")\n",
    "\n",
    "        # Specify maximum number of tokens if not setted\n",
    "        if num_tokens is None:\n",
    "            num_tokens = num_words + len(token2index)\n",
    "\n",
    "        progress_bar = tqdm(total=num_tokens-len(token2index), disable=not verbose, desc='Reading embeddings file')\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            token = ' '.join(parts[:-emb_dim]).lower()\n",
    "            if token in token2index:\n",
    "                continue\n",
    "            word_vector = np.array(list(map(float, parts[-emb_dim:]))).reshape(1, -1)\n",
    "            token2index[token] = len(token2index)\n",
    "            embeddings_matrix = np.append(embeddings_matrix, word_vector, axis=0)\n",
    "            progress_bar.update()\n",
    "            if len(token2index) == num_tokens:\n",
    "                break\n",
    "\n",
    "        progress_bar.close()\n",
    "        assert(len(token2index) == embeddings_matrix.shape[0])\n",
    "\n",
    "    return token2index, embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91f9f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding matrix with shape (2, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading embeddings file: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 998/998 [00:00<00:00, 1123.47it/s]\n"
     ]
    }
   ],
   "source": [
    "token2index, embeddings_matrix = load_embeddings(\n",
    "    path=\"cc.ru.300.vec\",\n",
    "    num_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b2b68",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "–ú—ã —Å—Ä–∞–∑—É –ø–æ–ª—É—á–∏–º `torch.utils.data.Dataset`, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –≤ `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e54fdaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (/Users/a18692338/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Reusing dataset tweet_eval (/Users/a18692338/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
      "Reusing dataset tweet_eval (/Users/a18692338/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"tweet_eval\"\n",
    "dataset_name = \"sentiment\"\n",
    "\n",
    "train_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"train\")\n",
    "valid_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"validation\")\n",
    "test_dataset = load_dataset(path=dataset_path, name=dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a0650",
   "metadata": {},
   "source": [
    "## `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dc742027",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9012ae5",
   "metadata": {},
   "source": [
    "## –ü–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –æ—Ç–¥–∞–µ—Ç –Ω–∞–º `Loader`\n",
    "–≠—Ç–æ –±–∞—Ç—á —Ñ–æ—Ä–º–∞—Ç–∞:\n",
    "```python\n",
    "batch = {\n",
    "    \"text\": [\n",
    "        \"text1\",\n",
    "        \"text2\",\n",
    "        ...,\n",
    "        \"textn\"\n",
    "    ],\n",
    "    \"label\": tensor([\n",
    "        1,\n",
    "        1,\n",
    "        ...,\n",
    "        0\n",
    "    ])\n",
    "}\n",
    "```\n",
    "–¢–æ –µ—Å—Ç—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å –¥–≤—É–º—è –∫–ª—é—á–∞–º–∏ `text` –∏ `label`, –≥–¥–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è n –ø—Ä–∏–º–µ—Ä–æ–≤. –¢–æ –µ—Å—Ç—å –¥–ª—è 5-–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"text\"][5]`, –∞ –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞ –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ `batch[\"label\"][5]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49bf6b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': tensor([0, 1]),\n",
       " 'text': ['\"Israel\\\\u2019s red line on Iran: 240 kg [of 20% enriched uranium] // Number was put to Netanyahu and he \\\\\"\"\"\"did not contest it\\\\\"\"\"\".',\n",
       "  '#rivalryweek 2 out of 4 today Mets beat the phillies Yankees lose to the Red Sox on a 9th inning 2-out grand slam robbery']}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9f0e6",
   "metadata": {},
   "source": [
    "## Collate\n",
    "–°–µ–π—á–∞—Å –ø–µ—Ä–µ–¥ –Ω–∞–º–∏ —Å—Ç–æ–∏—Ç –ø—Ä–æ–±–ª–µ–º–∞: –º—ã –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫, –∞ –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–µ–Ω–∑–æ—Ä—ã (–º–∞—Ç—Ä–∏—Ü—ã) —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, –∫ —Ç–æ–º—É –∂–µ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–∞–ø–∞–¥–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –≤—Å–µ —Å–ª–æ–∂–∏—Ç—å –≤ —Ç–æ—Ä—á–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É. –ú—ã –º–æ–∂–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏:\n",
    "- –î–æ—Å—Ç–∞—Ç—å –∏–∑ `train/valid/test_dataset` –¥–∞–Ω–Ω—ã–µ –∏ –Ω–∞–ø–∏—Å–∞—Ç—å —Å–≤–æ–π `Dataset`, –≥–¥–µ –≤–Ω—É—Ç—Ä–∏ –±—É–¥–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, —Ç–æ–∫–µ–Ω—ã –±—É–¥—É—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å—Å—è –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –∑–∞—Ç–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±—É–¥–µ—Ç –ø–∞–¥–∏—Ç—å—Å—è –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "- –°–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –±—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏ –Ω–∞—à–∏ –±–∞—Ç—á–∏. –û–Ω–∞ –≤—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –≤ `DataLoader(collate_fn=<–í–ê–®–ê_–§–£–ù–ö–¶–ò–Ø>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b0f2e",
   "metadata": {},
   "source": [
    "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Å–≤–æ–π `Dataset`\n",
    "–¢–æ –≤—ã –º–æ–∂–µ—Ç–µ –¥–æ—Å—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26771f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45615, 45615)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[\"text\"]), len(train_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f25200be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"',\n",
       " '\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"text\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d4c952b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"label\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68184652",
   "metadata": {},
   "source": [
    "## –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å `collate_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dab4d",
   "metadata": {},
   "source": [
    "### –î–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –≤–æ–æ–±—â–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
    "–î–ª—è —ç—Ç–æ–≥–æ —Å–¥–µ–ª–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é `empty_collate`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –±–∞—Ç—á –∏ –æ—Ç–¥–∞–µ—Ç –µ–≥–æ, –Ω–∏—á–µ–≥–æ —Å –Ω–∏–º –Ω–µ –¥–µ–ª–∞—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3b7ce86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_collate(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "26f0fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=empty_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=empty_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "942cf78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 1,\n",
       "  'text': '@user hey Curtis, could you please inbox me a quote about how NT hockey is doing so far for my NT Sun story.thanx!\"'},\n",
       " {'label': 1, 'text': 'The heat is gunna kill the Knicks tomorrow'}]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca5081",
   "metadata": {},
   "source": [
    "## –§–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞\n",
    "```python\n",
    "batch = [\n",
    "    {\n",
    "        \"text\": \"text1\",\n",
    "        \"label\": 0\n",
    "    }, \n",
    "    {\n",
    "        \"text\": \"text2\",\n",
    "        \"label\": 1\n",
    "    },\n",
    "    ...,\n",
    "    {\n",
    "        \"text\": \"textn\",\n",
    "        \"label\": 1\n",
    "    }\n",
    "]\n",
    "```\n",
    "–¢–æ –µ—Å—Ç—å —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç ‚Äî —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ `text` –∏ `label`.  \n",
    "\n",
    "–í—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∏–ª–∏ –∫–ª–∞—Å—Å —Å –º–µ—Ç–æ–¥–æ–º `collate`. –≠—Ç–æ—Ç —Å–ø–æ—Å–æ–± —Ä–µ—à–µ–Ω–∏—è –¥–æ–º–∞—à–∫–∏ –ø—Ä–µ–¥–æ–¥—á—Ç–∏—Ç–µ–ª—å–Ω–µ–π, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `collate` –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞.\n",
    "\n",
    "–ß—Ç–æ —è –ø—Ä–µ–¥–ª–∞–≥–∞—é:\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ –∫–ª–∞—Å—Å `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3b9ddcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self, base_tokenizer, token2index, pad_token, unk_token, max_length):\n",
    "        \n",
    "        self._base_tokenizer = base_tokenizer  # –Ω–∞–ø—Ä–∏–º–µ—Ä ToktokTokenizer()\n",
    "        \n",
    "        self.token2index = token2index  # —Å–ª–æ–≤–∞—Ä—å –∏–∑ load_embeddings()\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self.token2index[self.pad_token]\n",
    "        \n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = self.token2index[self.unk_token]\n",
    "        \n",
    "        self.max_length = max_length\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫–µ–Ω—ã\n",
    "        \"\"\"\n",
    "        return self._base_tokenizer.tokenize(text)\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ø–∏—Å–æ–∫ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        \"\"\"\n",
    "        token_ids = []\n",
    "        for token in tokenized_text:\n",
    "            token_id = token2index.get(token, self.unk_index)\n",
    "            token_ids.append(token_id)\n",
    "        return token_ids\n",
    "        \n",
    "    def padding(self, tokens_indices):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–ª–∏–Ω—É tokens_indices —Ä–∞–≤–Ω–æ–π self.max_length\n",
    "        –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è unk'–∏\n",
    "        \"\"\"\n",
    "        # TODO remove repeating unk indices.\n",
    "        \n",
    "        if len(tokens_indices) > self.max_length:\n",
    "            return tokens_indices[:self.max_length]\n",
    "        \n",
    "        pad_ids = [self.pad_index] * (self.max_length - len(tokens_indices))\n",
    "        tokens_indices.extend(pad_ids)\n",
    "        return tokens_indices\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        –í —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Å—Ç—Ä–æ–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤ –≤–µ–∫—Ç–æ—Ä —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Å–ª–æ–≤ –Ω—É–∂–Ω–æ —Ä–∞–∑–º–µ—Ä–∞ (self.max_length)\n",
    "        \"\"\"\n",
    "        text_tokenized = self.tokenize(text)\n",
    "        text_indexed = self.indexing(text_tokenized)\n",
    "        text_padded = self.padding(text_indexed)\n",
    "        return text_padded\n",
    "        \n",
    "    def collate(self, batch):\n",
    "        \n",
    "        tokenized_texts = list()\n",
    "        labels = list()\n",
    "        \n",
    "        for sample in batch:\n",
    "            label = sample[\"label\"]\n",
    "            text = sample[\"text\"]\n",
    "            \n",
    "            text_processed = self(text)\n",
    "            \n",
    "            tokenized_texts.append(text_processed)\n",
    "            labels.append(label)\n",
    "            \n",
    "        tokenized_texts = torch.tensor(tokenized_texts)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
    "        labels = torch.tensor(labels)  # –ø–µ—Ä–µ–≤–æ–¥ –≤ torch.Tensor\n",
    "        \n",
    "        return tokenized_texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "27233689",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    base_tokenizer=ToktokTokenizer(),\n",
    "    token2index=token2index,\n",
    "    pad_token=\"<PAD>\",\n",
    "    unk_token=\"<UNK>\",\n",
    "    max_length=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6b094d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 1, 1, 1, 1, 1, 667, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 22]\n"
     ]
    }
   ],
   "source": [
    "text = batch[0][\"text\"]\n",
    "print(tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab7e6d",
   "metadata": {},
   "source": [
    "## –ü–µ—Ä–µ–¥ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞\n",
    "–°–æ–≤–µ—Ç—É—é, —á—Ç–æ–±—ã –≤ –∏—Ç–æ–≥–µ `Loader` –æ—Ç–¥–∞–≤–∞–ª –∫–æ—Ä—Ç–µ–∂ —Å –¥–≤—É–º—è —Ç–µ–Ω–∑–æ—Ä–∞–º–∏:\n",
    "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–æ–∫–µ–Ω–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size, sequence_length)`\n",
    "- `torch.Tensor` —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ —Ç–∞—Ä–≥–µ—Ç–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `(batch_size)`\n",
    "\n",
    "–¢–æ –µ—Å—Ç—å, —á—Ç–æ–±—ã –±—ã–ª–æ —Ç–∞–∫:\n",
    "```python\n",
    "for x, y in train_loader:\n",
    "    ...\n",
    "\n",
    ">> x\n",
    ">> tensor([[   37,  3889,   470,  ...,     0,     0,     0],\n",
    "           [ 1509,   581,   144,  ...,     0,     0,     0],\n",
    "           [ 1804,   893,  2457,  ...,     0,     0,     0],\n",
    "           ...,\n",
    "           [  170, 39526,  2102,  ...,     0,     0,     0],\n",
    "           [ 1217,   172, 28440,  ...,     0,     0,     0],\n",
    "           [   37,    56,   603,  ...,     0,     0,     0]])\n",
    "\n",
    ">> y\n",
    ">> tensor([1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1,\n",
    "           0, 2, 1, 1, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
    "           1, 0, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 0, 1, 0, 2, 1, 2, 2, 1, 0, 0, 2, 2,\n",
    "           2, 1, 2, 0, 2, 2, 0, 2, 0, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 0, 2, 2,\n",
    "           2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 1, 1, 1, 2, 2, 1, 2, 1,\n",
    "           2, 1, 1, 2, 2, 1, 1, 2])\n",
    "\n",
    ">> x.shape\n",
    ">> torch.Size([128, 64])\n",
    "\n",
    ">> y.shape\n",
    ">> torch.Size([128])\n",
    "```\n",
    "–ü—Ä–∏ —É—Å–ª–æ–≤–∏–∏, —á—Ç–æ –±–∞—Ç—á —Å–∞–π–∑ —Ä–∞–≤–µ–Ω 128, –∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–≤–Ω–∞ 64.\n",
    "\n",
    "## –ü–æ–º–Ω–∏—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018cf71c",
   "metadata": {},
   "source": [
    "## <–ú–µ—Å—Ç–æ –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "40268f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=tokenizer.collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, collate_fn=tokenizer.collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, collate_fn=tokenizer.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d293b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "87752ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(isinstance(x, torch.Tensor))\n",
    "assert(len(x.size()) == 2)\n",
    "\n",
    "assert(isinstance(y, torch.Tensor))\n",
    "assert(len(y.size()) == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc96104",
   "metadata": {},
   "source": [
    "# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è DAN\n",
    "\n",
    "–ù–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –ø–æ–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤\n",
    "\n",
    "–®–∞–≥–∏:\n",
    "- –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "- –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ `Multilayer Perceptron`\n",
    "    - –ù—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–∞–º–æ–º—É\n",
    "    \n",
    "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:\n",
    "- –î–æ–±–∞–≤—å—Ç–µ `nn.Dropout`, `nn.BatchNorm` –ø–æ –≤–∫—É—Å—É\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –ø–∞–¥–æ–≤\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç –±–µ—Ä—Ç–∞/—Ä–æ–±–µ—Ä—Ç—ã/—Ç–¥ (–∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –ø—Ä–æ —ç—Ç–æ –±—É–¥–µ—Ç —Ü–µ–ª—ã–π —Ç—É—Ç–æ—Ä–∏–∞–ª, –∞ –ø–æ–∫–∞ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –≤–∞–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —Å–∞–º–∏–º)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123324f0",
   "metadata": {},
   "source": [
    "# –°–æ–≤–µ—Ç—ã\n",
    "\n",
    "## –î–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ—á–µ–º—É –æ–Ω–∞(–æ–Ω–∏)\n",
    "    - –û–±—ã—á–Ω–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏—è –∫–∞–∫–∏–µ –≤–µ—Å–∞ –±—Ä–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø–æ–º–æ–≥—É—Ç –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä, –æ —Ç–æ–º –≤—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –Ω–∞—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ö–æ—Ä–æ—à–æ –ª–∏ –º–æ–¥–µ–ª—å —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤ –∏ —Ç–¥\n",
    "- –≠—Ç—É –¥–æ–º–∞—à–∫—É –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –∏ –Ω–∞ `CPU`, –Ω–æ –Ω–∞ `GPU` –±—É–¥–µ—Ç —Å–∏–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ\n",
    "    - –í–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–æ–º–∞—à–∫–∞—Ö –º—ã –±—É–¥–µ–º —É—á–∏—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ `GPU`\n",
    "    - –†–∞–Ω–æ –∏–ª–∏ –ø–æ–∑–¥–Ω–æ –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —ç—Ç–æ—Ç [—Ç—É—Ç–æ—Ä–∏–∞–ª](https://www.youtube.com/watch?v=pgk1zGv5lU4)\n",
    "    - –í—ã –º–æ–∂–µ—Ç–µ –æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ `colab`, —ç—Ç–æ –±–µ—Å–ø–ª–∞—Ç–Ω–æ\n",
    "\n",
    "## –î–æ —ç–ø–æ—Ö–∏\n",
    "- –°–¥–µ–ª–∞–π—Ç–µ —Å–ø–∏—Å–∫–∏/—Å–ª–æ–≤–∞—Ä–∏/–¥—Ä—É–≥–æ–µ, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –Ω—É–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫(–∏) –ø–æ –≤—Å–µ–π —ç–ø–æ—Ö–µ –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –í–æ –≤—Ä–µ–º—è —ç–ø–æ—Ö–∏\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ [`tqdm`](https://github.com/tqdm/tqdm) –∫–∞–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –∫–∞–∫ –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤–∞—à–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –ª–æ—Å—Å\n",
    "- –õ–æ–≥–∏—Ä—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫—É(–∫–∏) –ø–æ –±–∞—Ç—á—É\n",
    "- –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ç–æ, —á—Ç–æ –≤–∞–º –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –ü–æ—Å–ª–µ —ç–ø–æ—Ö–∏\n",
    "- –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫(–∏) –Ω–∞ –≤—Å—é —ç–ø–æ—Ö—É –¥–ª—è —Ç—Ä–µ–π–Ω–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "\n",
    "## –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –ü—Ä–æ–≤–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –∏ –ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)\n",
    "- –ü–æ—Å—Ç—Ä–æ–π—Ç–µ –≥—Ä–∞—Ñ–∏–∫–∏:\n",
    "    - [Confusion Matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix)\n",
    "    - [–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ (—Ç–æ –µ—Å—Ç—å –¥–ª—è –∫–∞–∫–æ–≥–æ-—Ç–æ –ø—Ä–∏–º–µ—Ä–∞ –º—ã –≤—ã–±–∏—Ä–∞–µ–º —Ç–∞–∫–æ–π –∫–ª–∞—Å—Å –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —ç—Ç–æ–≥–æ –≤—ã–±–æ—Ä–∞ —Ç–∞–∫–∞—è-—Ç–æ) –Ω–∞ —Ç—Ä–µ–π–Ω–µ/—Ç–µ—Å—Ç–µ/–≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        - –ï—Å–ª–∏ –∫–ª–∞—Å—Å –±—ã–ª –≤—ã–±—Ä–∞–Ω –≤–µ—Ä–Ω–æ –∏ –µ—Å–ª–∏ –±—ã–ª–∞ –æ—à–∏–±–∫–∞\n",
    "- –ü–æ–¥—É–º–∞–π—Ç–µ —á—Ç–æ –µ—â–µ –≤–∞–º –±—É–¥–µ—Ç –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —Ç–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã: \n",
    "    - –ß—Ç–æ –≤ –º–æ–¥–µ–ª–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?\n",
    "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –º–æ–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏?\n",
    "    - –í—Å–µ –ª–∏ —Ö–æ—Ä–æ—à–æ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π?\n",
    "    - –ù–µ –ø–µ—Ä–µ–æ–±—É—á–∏–ª—Å—è –ª–∏ —è?\n",
    "    - –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —è –ø–æ—Å–º–æ—Ç—Ä–µ–ª –Ω–∞ –¥–∞–Ω–Ω—ã–µ?\n",
    "    - –ù—É–∂–Ω–æ –ª–∏ –º–Ω–µ —É–ª—É—á—à–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö?\n",
    "    - –ù—É–∂–Ω–æ –ª–∏ –ø–æ–º–µ–Ω—è—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏?\n",
    "    - –ù–µ—Ç –ª–∏ —É –º–µ–Ω—è –±–∞–≥–æ–≤ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏?\n",
    "    - –ö–∞–∫–∏–µ —Ç–∏–ø–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏ —É –º–æ–µ–π –º–æ–¥–µ–ª–∏?\n",
    "    - –ö–∞–∫ —è –º–æ–≥—É –∏—Ö –∏—Å–ø—Ä–∞–≤–∏—Ç—å?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1f451c",
   "metadata": {},
   "source": [
    "# –Ø –≤—ã–±—Ä–∞–ª –º–µ—Ç—Ä–∏–∫—É <–ú–ï–¢–†–ò–ö–ê>\n",
    "–ü–æ—á–µ–º—É —è –≤—ã–±—Ä–∞–ª —ç—Ç—É –º–µ—Ç—Ä–∏–∫—É:  \n",
    "<–†–ê–°–°–ö–ê–ó_–ü–†–û_–ú–ï–¢–†–ò–ö–£>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da42260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAverageNetwork(nn.Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7966f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepAverageNetwork(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b4a2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efad1d1",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34c552fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628847fe",
   "metadata": {},
   "source": [
    "## –°–¥–µ–ª–∞–π—Ç–µ —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b634b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 2  # –ó–∞–¥–∞–π—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "\n",
    "...\n",
    "\n",
    "for n_epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    # train\n",
    "    ...\n",
    "\n",
    "    # validation\n",
    "    ...\n",
    "    \n",
    "# test\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87583e79",
   "metadata": {},
   "source": [
    "# –í—ã–≤–æ–¥—ã\n",
    "–ù–∞–ø–∏—à–∏—Ç–µ –Ω–µ–±–æ–ª—å—à–æ–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ. –ß—Ç–æ —É–¥–∞–ª–æ—Å—å, –≤ —á–µ–º –Ω–µ —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad7f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_boba_hw2",
   "language": "python",
   "name": "dl_boba_hw2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
